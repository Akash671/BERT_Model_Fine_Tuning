{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "48f45ba9ac9b45829be4a65345cdc5b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_12e041cb451940b297c1ab176558335f",
              "IPY_MODEL_8323c798cee04a63afb69faa0c945194",
              "IPY_MODEL_cba8efcf913f4bd0a319b971d672baef"
            ],
            "layout": "IPY_MODEL_2d17f24cacaf42c8921656cc2d6d7930"
          }
        },
        "12e041cb451940b297c1ab176558335f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a172802b16a74c2fb9c1cef072a67fe1",
            "placeholder": "​",
            "style": "IPY_MODEL_026eaeb746d84c759401a2537d3924bc",
            "value": "Map: 100%"
          }
        },
        "8323c798cee04a63afb69faa0c945194": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af8d21a244ec45578bec14ef09cddc9d",
            "max": 800,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_490ad7125abd4718ab9fac9be532229d",
            "value": 800
          }
        },
        "cba8efcf913f4bd0a319b971d672baef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e15643cfc3204efbbff38f1dd6c9d125",
            "placeholder": "​",
            "style": "IPY_MODEL_473f25c621bd4cc1bcb16c38d687517f",
            "value": " 800/800 [00:00&lt;00:00, 1079.78 examples/s]"
          }
        },
        "2d17f24cacaf42c8921656cc2d6d7930": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a172802b16a74c2fb9c1cef072a67fe1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "026eaeb746d84c759401a2537d3924bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "af8d21a244ec45578bec14ef09cddc9d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "490ad7125abd4718ab9fac9be532229d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e15643cfc3204efbbff38f1dd6c9d125": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "473f25c621bd4cc1bcb16c38d687517f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8a6950fe4169485baaef88c14036fbad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_990b3e1f8d44483f89ee746032cad44e",
              "IPY_MODEL_45234e0067a44337abb41159955e130b",
              "IPY_MODEL_d94a9b988e1947ed9a9fb5c2352df419"
            ],
            "layout": "IPY_MODEL_8b041ec7a0d2412caf3e81d55bd883e7"
          }
        },
        "990b3e1f8d44483f89ee746032cad44e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7671c929f48d46beb0fda199c65c7632",
            "placeholder": "​",
            "style": "IPY_MODEL_a73fd367a0644fe286282b8566cfa58f",
            "value": "Map: 100%"
          }
        },
        "45234e0067a44337abb41159955e130b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f0f6862869ca4a3f82be39f65f3b5071",
            "max": 200,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b245090790894a209d2cd164239bf17a",
            "value": 200
          }
        },
        "d94a9b988e1947ed9a9fb5c2352df419": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_674da6239f9c4e35aca1c8789bb0278c",
            "placeholder": "​",
            "style": "IPY_MODEL_c07844217c4b4cf7a6f46aa458eecf3c",
            "value": " 200/200 [00:00&lt;00:00, 1016.58 examples/s]"
          }
        },
        "8b041ec7a0d2412caf3e81d55bd883e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7671c929f48d46beb0fda199c65c7632": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a73fd367a0644fe286282b8566cfa58f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f0f6862869ca4a3f82be39f65f3b5071": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b245090790894a209d2cd164239bf17a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "674da6239f9c4e35aca1c8789bb0278c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c07844217c4b4cf7a6f46aa458eecf3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "\n",
        "def generate_fake_qa_dataset(num_samples=1000):\n",
        "    \"\"\"\n",
        "    Generates a fake dataset for Question Answering (Q&A) fine-tuning.\n",
        "\n",
        "    Each sample consists of a context, a question related to the context,\n",
        "    and an answer extracted from the context.\n",
        "    \"\"\"\n",
        "    dataset = []\n",
        "    base_contexts = [\n",
        "        \"The capital of France is Paris. Paris is also known as the City of Lights. It is famous for the Eiffel Tower.\",\n",
        "        \"Mount Everest is the highest mountain in the world, located in the Himalayas. Its peak is 8,848.86 meters above sea level.\",\n",
        "        \"The Amazon River, located in South America, is the largest river by discharge volume of water in the world.\",\n",
        "        \"Photosynthesis is the process used by plants, algae and cyanobacteria to convert light energy into chemical energy.\",\n",
        "        \"Artificial intelligence (AI) is intelligence demonstrated by machines, unlike the natural intelligence displayed by humans and animals.\",\n",
        "        \"The Internet is a global system of interconnected computer networks that uses the Internet protocol suite (TCP/IP) to communicate.\",\n",
        "        \"Renewable energy sources, like solar and wind power, are naturally replenished on a human timescale.\",\n",
        "        \"The human heart is a muscular organ that pumps blood through the circulatory system, supplying oxygen and nutrients to the body.\",\n",
        "        \"Quantum computing is a type of computation that harnesses the phenomena of quantum mechanics, such as superposition and entanglement.\",\n",
        "        \"Machine learning is a subset of artificial intelligence that involves training algorithms to learn from data and make predictions or decisions.\",\n",
        "    ]\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        context = random.choice(base_contexts)\n",
        "        question = \"\"\n",
        "        answer = \"\"\n",
        "\n",
        "        # Simple logic to create questions and answers based on keywords in contexts\n",
        "        if \"Paris\" in context and \"France\" in context:\n",
        "            question = f\"What is the capital of France? (Sample {i+1})\"\n",
        "            answer = \"Paris\"\n",
        "        elif \"Mount Everest\" in context and \"highest mountain\" in context:\n",
        "            question = f\"What is the highest mountain in the world? (Sample {i+1})\"\n",
        "            answer = \"Mount Everest\"\n",
        "        elif \"Amazon River\" in context and \"South America\" in context:\n",
        "            question = f\"Where is the Amazon River located? (Sample {i+1})\"\n",
        "            answer = \"South America\"\n",
        "        elif \"Photosynthesis\" in context and \"plants\" in context:\n",
        "            question = f\"What process do plants use to convert light energy? (Sample {i+1})\"\n",
        "            answer = \"Photosynthesis\"\n",
        "        elif \"Artificial intelligence\" in context and \"machines\" in context:\n",
        "            question = f\"What is intelligence demonstrated by machines called? (Sample {i+1})\"\n",
        "            answer = \"Artificial intelligence\"\n",
        "        elif \"Internet\" in context and \"global system\" in context:\n",
        "            question = f\"What is a global system of interconnected computer networks? (Sample {i+1})\"\n",
        "            answer = \"The Internet\"\n",
        "        elif \"Renewable energy\" in context and \"solar\" in context:\n",
        "            question = f\"Name a type of renewable energy. (Sample {i+1})\"\n",
        "            answer = \"solar\"\n",
        "        elif \"human heart\" in context and \"pumps blood\" in context:\n",
        "            question = f\"What organ pumps blood through the circulatory system? (Sample {i+1})\"\n",
        "            answer = \"human heart\"\n",
        "        elif \"Quantum computing\" in context and \"quantum mechanics\" in context:\n",
        "            question = f\"What type of computation harnesses quantum mechanics? (Sample {i+1})\"\n",
        "            answer = \"Quantum computing\"\n",
        "        elif \"Machine learning\" in context and \"artificial intelligence\" in context:\n",
        "            question = f\"What is a subset of artificial intelligence that learns from data? (Sample {i+1})\"\n",
        "            answer = \"Machine learning\"\n",
        "        else:\n",
        "            # Fallback for less specific contexts or if no specific match\n",
        "            question = f\"Tell me something about {context.split(' ')[random.randint(0, len(context.split())-1)]}? (Sample {i+1})\"\n",
        "            answer_words = context.split()\n",
        "            answer_start = random.randint(0, len(answer_words) - 5) # Ensure enough words for a snippet\n",
        "            answer = \" \".join(answer_words[answer_start:answer_start + random.randint(2, 5)])\n",
        "\n",
        "\n",
        "        dataset.append({\n",
        "            \"id\": str(i), # Unique ID for each sample\n",
        "            \"context\": context,\n",
        "            \"question\": question,\n",
        "            \"answer\": {\n",
        "                \"text\": answer,\n",
        "                # For BERT fine-tuning, you often need the start character position of the answer in the context\n",
        "                \"answer_start\": context.find(answer) if answer in context else -1\n",
        "            }\n",
        "        })\n",
        "\n",
        "    return dataset\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    fake_qa_data = generate_fake_qa_dataset(num_samples=1000)\n",
        "    print(f\"Generated {len(fake_qa_data)} samples.\")\n",
        "\n",
        "    # You can save this to a JSON file\n",
        "    with open(\"fake_qa_dataset.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(fake_qa_data, f, indent=4)\n",
        "    print(\"Dataset saved to fake_qa_dataset.json\")\n",
        "\n",
        "    # Print a few samples to verify\n",
        "    print(\"\\n--- First 3 Samples ---\")\n",
        "    for j in range(min(3, len(fake_qa_data))):\n",
        "        print(f\"Sample {j+1}:\")\n",
        "        print(f\"  Context: {fake_qa_data[j]['context']}\")\n",
        "        print(f\"  Question: {fake_qa_data[j]['question']}\")\n",
        "        print(f\"  Answer: {fake_qa_data[j]['answer']['text']} (Start: {fake_qa_data[j]['answer']['answer_start']})\")\n",
        "        print(\"-\" * 20)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPtSA9f7IvaI",
        "outputId": "9a678483-30bb-43e6-8d81-db300f512c20"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 1000 samples.\n",
            "Dataset saved to fake_qa_dataset.json\n",
            "\n",
            "--- First 3 Samples ---\n",
            "Sample 1:\n",
            "  Context: Artificial intelligence (AI) is intelligence demonstrated by machines, unlike the natural intelligence displayed by humans and animals.\n",
            "  Question: What is intelligence demonstrated by machines called? (Sample 1)\n",
            "  Answer: Artificial intelligence (Start: 0)\n",
            "--------------------\n",
            "Sample 2:\n",
            "  Context: Machine learning is a subset of artificial intelligence that involves training algorithms to learn from data and make predictions or decisions.\n",
            "  Question: What is a subset of artificial intelligence that learns from data? (Sample 2)\n",
            "  Answer: Machine learning (Start: 0)\n",
            "--------------------\n",
            "Sample 3:\n",
            "  Context: Machine learning is a subset of artificial intelligence that involves training algorithms to learn from data and make predictions or decisions.\n",
            "  Question: What is a subset of artificial intelligence that learns from data? (Sample 3)\n",
            "  Answer: Machine learning (Start: 0)\n",
            "--------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Etvm3opnIvcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from datasets import Dataset, DatasetDict # Using Hugging Face's datasets library\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# --- 1. Load the dataset ---\n",
        "def load_qa_dataset(file_path=\"fake_qa_dataset.json\"):\n",
        "    \"\"\"\n",
        "    Loads the fake Q&A dataset from a JSON file.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            data = json.load(f)\n",
        "        print(f\"Successfully loaded dataset from {file_path}. Found {len(data)} samples.\")\n",
        "        return data\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Dataset file not found at {file_path}. Please run the data generation script first.\")\n",
        "        return None\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"Error: Could not decode JSON from {file_path}. Check file format.\")\n",
        "        return None\n",
        "\n",
        "# Load your dataset\n",
        "raw_datasets = load_qa_dataset()\n",
        "\n",
        "if raw_datasets is None:\n",
        "    exit() # Exit if dataset loading failed\n",
        "\n",
        "# Convert the list of dictionaries to a Hugging Face Dataset object\n",
        "# For simplicity, we'll put all data into the 'train' split for this example.\n",
        "# In a real scenario, you'd split into train, validation, and test.\n",
        "hf_dataset = Dataset.from_list(raw_datasets)\n",
        "# Create a DatasetDict if you want to define splits (e.g., train, validation)\n",
        "# For this example, let's create a small train/test split (80/20)\n",
        "train_test_split = hf_dataset.train_test_split(test_size=0.2, seed=42)\n",
        "dataset_dict = DatasetDict({\n",
        "    'train': train_test_split['train'],\n",
        "    'validation': train_test_split['test'] # Using test set as validation for this example\n",
        "})\n",
        "print(f\"Dataset split into: {dataset_dict}\")\n",
        "\n",
        "\n",
        "# --- 2. Load pre-trained BERT model and tokenizer ---\n",
        "# You can choose different BERT-like models, e.g., 'bert-base-uncased', 'distilbert-base-uncased'\n",
        "model_checkpoint = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
        "\n",
        "print(f\"Loaded tokenizer and model: {model_checkpoint}\")\n",
        "\n",
        "# --- 3. Preprocess the dataset ---\n",
        "# This is the most crucial part for Q&A fine-tuning.\n",
        "# We need to map answers to token spans.\n",
        "\n",
        "max_length = 384  # The maximum length of a feature (context and question)\n",
        "doc_stride = 128   # The authorized overlap between two consecutive chunks\n",
        "\n",
        "def preprocess_training_examples(examples):\n",
        "    \"\"\"\n",
        "    Preprocesses the training examples for BERT Q&A.\n",
        "    This involves tokenization, handling long contexts, and finding answer spans.\n",
        "    \"\"\"\n",
        "    questions = [q.strip() for q in examples[\"question\"]]\n",
        "    contexts = examples[\"context\"]\n",
        "    answers = examples[\"answer\"]\n",
        "\n",
        "    # Tokenize contexts and questions together.\n",
        "    # `truncation=\"only_second\"` truncates the context if the combined length exceeds max_length.\n",
        "    # `return_offsets_mapping` is crucial for mapping token spans back to original text.\n",
        "    # `padding=\"max_length\"` pads to max_length.\n",
        "    # `stride` handles overlapping chunks for long contexts.\n",
        "    tokenized_examples = tokenizer(\n",
        "        questions,\n",
        "        contexts,\n",
        "        max_length=max_length,\n",
        "        truncation=\"only_second\",\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "        stride=doc_stride,\n",
        "    )\n",
        "\n",
        "    # Since one example can give us several features if it has a long context,\n",
        "    # we need to ensure that each feature has the correct `example_id`\n",
        "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "    # The offsets mapping will give us a tuple of (start_char, end_char) for each token.\n",
        "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
        "\n",
        "    tokenized_examples[\"start_positions\"] = []\n",
        "    tokenized_examples[\"end_positions\"] = []\n",
        "\n",
        "    for i, offsets in enumerate(offset_mapping):\n",
        "        # We find the example that this feature came from\n",
        "        sample_idx = sample_mapping[i]\n",
        "        answer = answers[sample_idx]\n",
        "        context = contexts[sample_idx]\n",
        "\n",
        "        # Start and end character positions of the answer in the original context\n",
        "        start_char = answer[\"answer_start\"]\n",
        "        end_char = start_char + len(answer[\"text\"])\n",
        "\n",
        "        # Sequence ID tells us if a token belongs to the question (0) or context (1)\n",
        "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "\n",
        "        # Find the start and end of the context in the tokenized input\n",
        "        # This is where the context begins after the [CLS] and question tokens\n",
        "        idx = 0\n",
        "        while sequence_ids[idx] != 1: # Find first token of context (sequence_id = 1)\n",
        "            idx += 1\n",
        "        context_start_token = idx\n",
        "\n",
        "        # Find the end of the context\n",
        "        idx = len(sequence_ids) - 1\n",
        "        while sequence_ids[idx] != 1: # Find last token of context (sequence_id = 1)\n",
        "            idx -= 1\n",
        "        context_end_token = idx\n",
        "\n",
        "        # If the answer is not fully contained in this chunk, set positions to 0 (CLS token)\n",
        "        # This is a common practice when the answer is not found or spans across chunks.\n",
        "        # The model will learn to predict the [CLS] token in such cases, indicating no answer.\n",
        "        if (offsets[context_start_token][0] > start_char or\n",
        "            offsets[context_end_token][1] < end_char):\n",
        "            tokenized_examples[\"start_positions\"].append(0)\n",
        "            tokenized_examples[\"end_positions\"].append(0)\n",
        "        else:\n",
        "            # Otherwise, find the start and end token positions\n",
        "            # Iterate over tokens and check if their character offsets overlap with the answer.\n",
        "            token_start_index = context_start_token\n",
        "            while token_start_index <= context_end_token and offsets[token_start_index][0] <= start_char:\n",
        "                token_start_index += 1\n",
        "            tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
        "\n",
        "            token_end_index = context_end_token\n",
        "            while token_end_index >= context_start_token and offsets[token_end_index][1] >= end_char:\n",
        "                token_end_index -= 1\n",
        "            tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
        "    return tokenized_examples\n",
        "\n",
        "# Apply preprocessing to the dataset\n",
        "print(\"Preprocessing training examples...\")\n",
        "tokenized_datasets = dataset_dict.map(\n",
        "    preprocess_training_examples,\n",
        "    batched=True,\n",
        "    remove_columns=dataset_dict[\"train\"].column_names # Remove original columns not needed for training\n",
        ")\n",
        "print(\"Preprocessing complete.\")\n",
        "\n",
        "# --- 4. Define training arguments ---\n",
        "# Define output directory for saving checkpoints\n",
        "output_dir = \"./results\"\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    eval_strategy=\"epoch\", # CORRECTED: Use eval_strategy instead of evaluation_strategy\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8, # Adjust based on your GPU memory\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3, # Number of epochs to train for\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\", # Save model every epoch\n",
        "    load_best_model_at_end=True, # Load the best model at the end of training\n",
        "    metric_for_best_model=\"eval_loss\", # Metric to use for determining the best model\n",
        "    push_to_hub=False, # Set to True if you want to push to Hugging Face Hub (requires login)\n",
        "    do_train=True, # Explicitly enable training\n",
        "    do_eval=True,  # Explicitly enable evaluation\n",
        ")\n",
        "\n",
        "# --- 5. Initialize Trainer ---\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer, # Pass tokenizer to Trainer for proper saving\n",
        ")\n",
        "\n",
        "# --- 6. Train the model ---\n",
        "print(\"Starting model training...\")\n",
        "trainer.train()\n",
        "print(\"Training complete!\")\n",
        "\n",
        "# Save the fine-tuned model and tokenizer\n",
        "model_save_path = \"./fine_tuned_bert_qa\"\n",
        "if not os.path.exists(model_save_path):\n",
        "    os.makedirs(model_save_path)\n",
        "\n",
        "trainer.save_model(model_save_path)\n",
        "tokenizer.save_pretrained(model_save_path)\n",
        "print(f\"Fine-tuned model and tokenizer saved to {model_save_path}\")\n",
        "\n",
        "print(\"\\nYou can now load this model for inference:\")\n",
        "print(f\"\"\"\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"{model_save_path}\")\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"{model_save_path}\")\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 760,
          "referenced_widgets": [
            "48f45ba9ac9b45829be4a65345cdc5b4",
            "12e041cb451940b297c1ab176558335f",
            "8323c798cee04a63afb69faa0c945194",
            "cba8efcf913f4bd0a319b971d672baef",
            "2d17f24cacaf42c8921656cc2d6d7930",
            "a172802b16a74c2fb9c1cef072a67fe1",
            "026eaeb746d84c759401a2537d3924bc",
            "af8d21a244ec45578bec14ef09cddc9d",
            "490ad7125abd4718ab9fac9be532229d",
            "e15643cfc3204efbbff38f1dd6c9d125",
            "473f25c621bd4cc1bcb16c38d687517f",
            "8a6950fe4169485baaef88c14036fbad",
            "990b3e1f8d44483f89ee746032cad44e",
            "45234e0067a44337abb41159955e130b",
            "d94a9b988e1947ed9a9fb5c2352df419",
            "8b041ec7a0d2412caf3e81d55bd883e7",
            "7671c929f48d46beb0fda199c65c7632",
            "a73fd367a0644fe286282b8566cfa58f",
            "f0f6862869ca4a3f82be39f65f3b5071",
            "b245090790894a209d2cd164239bf17a",
            "674da6239f9c4e35aca1c8789bb0278c",
            "c07844217c4b4cf7a6f46aa458eecf3c"
          ]
        },
        "id": "WvO0wEDXIvfY",
        "outputId": "c63116b7-e7a8-4304-949e-7e17ac12ac5e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded dataset from fake_qa_dataset.json. Found 1000 samples.\n",
            "Dataset split into: DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['id', 'context', 'question', 'answer'],\n",
            "        num_rows: 800\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['id', 'context', 'question', 'answer'],\n",
            "        num_rows: 200\n",
            "    })\n",
            "})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded tokenizer and model: bert-base-uncased\n",
            "Preprocessing training examples...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "48f45ba9ac9b45829be4a65345cdc5b4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8a6950fe4169485baaef88c14036fbad"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing complete.\n",
            "Starting model training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-33-3606664691>:174: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [300/300 3:24:55, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.002300</td>\n",
              "      <td>0.000965</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.001400</td>\n",
              "      <td>0.000563</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.001100</td>\n",
              "      <td>0.000486</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training complete!\n",
            "Fine-tuned model and tokenizer saved to ./fine_tuned_bert_qa\n",
            "\n",
            "You can now load this model for inference:\n",
            "\n",
            "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
            "tokenizer = AutoTokenizer.from_pretrained(\"./fine_tuned_bert_qa\")\n",
            "model = AutoModelForQuestionAnswering.from_pretrained(\"./fine_tuned_bert_qa\")\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NKE-QBytIviI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n",
        "import torch\n",
        "import os\n",
        "\n",
        "# Define the path where your fine-tuned model and tokenizer are saved\n",
        "model_path = \"./fine_tuned_bert_qa\"\n",
        "\n",
        "# --- 1. Load the fine-tuned model and tokenizer ---\n",
        "print(f\"Loading fine-tuned model and tokenizer from: {model_path}\")\n",
        "if not os.path.exists(model_path):\n",
        "    print(f\"Error: Model directory not found at {model_path}. \"\n",
        "          \"Please ensure the fine-tuning script completed successfully \"\n",
        "          \"and saved the model to this location.\")\n",
        "    exit()\n",
        "\n",
        "try:\n",
        "    # We use Auto classes to ensure compatibility with the saved model\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    model = AutoModelForQuestionAnswering.from_pretrained(model_path)\n",
        "    print(\"Model and tokenizer loaded successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Failed to load model or tokenizer. Error: {e}\")\n",
        "    print(\"This might happen if the model was not saved correctly or if there's a version mismatch.\")\n",
        "    exit()\n",
        "\n",
        "# --- 2. Create a Question-Answering pipeline ---\n",
        "# The pipeline handles tokenization, model inference, and post-processing (extracting the answer span)\n",
        "# You can specify the device if you have a GPU (e.g., device=0 for the first GPU)\n",
        "qa_pipeline = pipeline(\n",
        "    \"question-answering\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=0 if torch.cuda.is_available() else -1 # Use GPU if available, else CPU\n",
        ")\n",
        "\n",
        "print(f\"QA pipeline initialized. Using device: {'GPU' if torch.cuda.is_available() else 'CPU'}\")\n",
        "\n",
        "# --- 3. Define a function for prediction ---\n",
        "def get_answer(question: str, context: str):\n",
        "    \"\"\"\n",
        "    Uses the fine-tuned BERT model to get an answer from a given context and question.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # The pipeline returns a dictionary with 'score', 'start', 'end', and 'answer'\n",
        "        result = qa_pipeline(question=question, context=context)\n",
        "        return result['answer']\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during prediction: {e}\")\n",
        "        return \"Could not find an answer.\"\n",
        "\n",
        "# --- 4. Test with example questions and contexts ---\n",
        "\n",
        "print(\"\\n--- Running Predictions ---\")\n",
        "\n",
        "# Example 1: Based on one of your generated contexts\n",
        "context1 = \"The capital of France is Paris. Paris is also known as the City of Lights. It is famous for the Eiffel Tower.\"\n",
        "question1 = \"What is the capital of France?\"\n",
        "print(f\"\\nContext: {context1}\")\n",
        "print(f\"Question: {question1}\")\n",
        "print(f\"Predicted Answer: {get_answer(question1, context1)}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Example 2: Another context from your generated data\n",
        "context2 = \"Mount Everest is the highest mountain in the world, located in the Himalayas. Its peak is 8,848.86 meters above sea level.\"\n",
        "question2 = \"Where is Mount Everest located?\"\n",
        "print(f\"\\nContext: {context2}\")\n",
        "print(f\"Question: {question2}\")\n",
        "print(f\"Predicted Answer: {get_answer(question2, context2)}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Example 3: A slightly different question to test generalization\n",
        "context3 = \"Photosynthesis is the process used by plants, algae and cyanobacteria to convert light energy into chemical energy.\"\n",
        "question3 = \"What do plants do with light energy?\"\n",
        "print(f\"\\nContext: {context3}\")\n",
        "print(f\"Question: {question3}\")\n",
        "print(f\"Predicted Answer: {get_answer(question3, context3)}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Example 4: A question for which the answer might not be directly in the context\n",
        "# (the model might return a less specific answer or an empty string depending on training)\n",
        "context4 = \"Artificial intelligence (AI) is intelligence demonstrated by machines, unlike the natural intelligence displayed by humans and animals.\"\n",
        "question4 = \"What is the future of AI?\" # This specific answer is not directly in the context\n",
        "print(f\"\\nContext: {context4}\")\n",
        "print(f\"Question: {question4}\")\n",
        "print(f\"Predicted Answer: {get_answer(question4, context4)}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\nPrediction process complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WU7TyiAw8rBQ",
        "outputId": "aa83b89d-54d0-44d1-8cc7-e1f3ff3c97e0"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading fine-tuned model and tokenizer from: ./fine_tuned_bert_qa\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model and tokenizer loaded successfully!\n",
            "QA pipeline initialized. Using device: CPU\n",
            "\n",
            "--- Running Predictions ---\n",
            "\n",
            "Context: The capital of France is Paris. Paris is also known as the City of Lights. It is famous for the Eiffel Tower.\n",
            "Question: What is the capital of France?\n",
            "Predicted Answer: Paris\n",
            "------------------------------\n",
            "\n",
            "Context: Mount Everest is the highest mountain in the world, located in the Himalayas. Its peak is 8,848.86 meters above sea level.\n",
            "Question: Where is Mount Everest located?\n",
            "Predicted Answer: Mount Everest\n",
            "------------------------------\n",
            "\n",
            "Context: Photosynthesis is the process used by plants, algae and cyanobacteria to convert light energy into chemical energy.\n",
            "Question: What do plants do with light energy?\n",
            "Predicted Answer: Photosynthesis\n",
            "------------------------------\n",
            "\n",
            "Context: Artificial intelligence (AI) is intelligence demonstrated by machines, unlike the natural intelligence displayed by humans and animals.\n",
            "Question: What is the future of AI?\n",
            "Predicted Answer: Artificial intelligence\n",
            "------------------------------\n",
            "\n",
            "Prediction process complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context4 = \"Sam Altman is the CEO of OpenAI and he is also co-founder of OpenAI\"\n",
        "question4 = \"who is the ceo of OpenAI?\" # This specific answer is not directly in the context\n",
        "print(f\"\\nContext: {context4}\")\n",
        "print(f\"Question: {question4}\")\n",
        "print(f\"Predicted Answer: {get_answer(question4, context4)}\")\n",
        "print(\"-\" * 30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5DGJQASu8rD8",
        "outputId": "9546934d-113c-434f-d0b1-d2909938b341"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Context: Sam Altman is the CEO of OpenAI and he is also co-founder of OpenAI\n",
            "Question: who is the ceo of OpenAI?\n",
            "Predicted Answer: OpenAI\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UXePm4lq8rIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F8gywNma8rKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1UG9tveY8rOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mQhQI2tJIvll"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}